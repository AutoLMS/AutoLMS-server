#!/usr/bin/env python3
"""
eClass í˜ì´ì§€ êµ¬ì¡° ìƒì„¸ ë¶„ì„
"""
import asyncio
import logging
from app.core.config import settings
from app.services.eclass_service import EclassService
from app.services.file_handler import FileHandler
from bs4 import BeautifulSoup
import httpx

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def analyze_eclass_structure():
    """eClass í˜ì´ì§€ êµ¬ì¡° ë¶„ì„"""
    try:
        logger.info("=== eClass í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ===")
        
        # eClass ë¡œê·¸ì¸
        file_handler = FileHandler()
        eclass_service = EclassService(file_handler=file_handler)
        await eclass_service.login(settings.ECLASS_USERNAME, settings.ECLASS_PASSWORD)
        
        # ì²« ë²ˆì§¸ ê°•ì˜ ë¶„ì„
        course_id = "A2025114608241001"  # Capstone Design I
        logger.info(f"ğŸ” ê°•ì˜ {course_id} ìƒì„¸ ë¶„ì„...")
        
        async with httpx.AsyncClient() as client:
            cookies = {}
            if hasattr(eclass_service, 'session'):
                cookies = eclass_service.session.cookies
            
            # 1. ë©”ì¸ ê°•ì˜ í˜ì´ì§€
            main_url = f"https://eclass.seoultech.ac.kr/ilos/st/course/submain_form.acl?KJKEY={course_id}&"
            response = await client.get(main_url, cookies=cookies)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                logger.info("ğŸ“‹ ë©”ì¸ í˜ì´ì§€ ë§í¬ë“¤ ë¶„ì„:")
                all_links = soup.find_all('a', href=True)
                
                # ë§í¬ ë¶„ë¥˜
                lesson_links = []
                material_links = []
                notice_links = []
                
                for link in all_links:
                    href = link.get('href')
                    text = link.get_text(strip=True)
                    
                    if href and text:
                        if 'lesson' in href:
                            lesson_links.append({'text': text, 'href': href})
                        elif 'material' in href or 'data' in href:
                            material_links.append({'text': text, 'href': href})
                        elif 'notice' in href:
                            notice_links.append({'text': text, 'href': href})
                
                logger.info(f"  ğŸ“š Lesson ë§í¬: {len(lesson_links)}ê°œ")
                for i, link in enumerate(lesson_links[:5]):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    logger.info(f"    {i+1}. {link['text']} -> {link['href']}")
                
                logger.info(f"  ğŸ“„ Material ë§í¬: {len(material_links)}ê°œ")
                for i, link in enumerate(material_links[:5]):
                    logger.info(f"    {i+1}. {link['text']} -> {link['href']}")
                
                logger.info(f"  ğŸ“¢ Notice ë§í¬: {len(notice_links)}ê°œ")
                for i, link in enumerate(notice_links[:5]):
                    logger.info(f"    {i+1}. {link['text']} -> {link['href']}")
                
                # 2. ê°•ì˜ìë£Œì‹¤ í˜ì´ì§€ í™•ì¸
                logger.info("\nğŸ“ ê°•ì˜ìë£Œì‹¤ í˜ì´ì§€ í™•ì¸...")
                data_url = f"https://eclass.seoultech.ac.kr/ilos/st/course/data_list.acl?KJKEY={course_id}&"
                
                data_response = await client.get(data_url, cookies=cookies)
                if data_response.status_code == 200:
                    data_soup = BeautifulSoup(data_response.text, 'html.parser')
                    
                    # í…Œì´ë¸”ì—ì„œ ìë£Œ ì°¾ê¸°
                    tables = data_soup.find_all('table')
                    logger.info(f"  í…Œì´ë¸” ìˆ˜: {len(tables)}")
                    
                    for i, table in enumerate(tables):
                        rows = table.find_all('tr')
                        if len(rows) > 1:  # í—¤ë”ê°€ ìˆëŠ” í…Œì´ë¸”
                            logger.info(f"  í…Œì´ë¸” {i+1}: {len(rows)}ê°œ í–‰")
                            
                            for j, row in enumerate(rows[:3]):  # ì²˜ìŒ 3ê°œ í–‰ë§Œ í™•ì¸
                                cells = row.find_all(['td', 'th'])
                                if cells:
                                    cell_texts = [cell.get_text(strip=True) for cell in cells]
                                    logger.info(f"    í–‰ {j+1}: {cell_texts}")
                                    
                                    # ì²¨ë¶€íŒŒì¼ ë§í¬ í™•ì¸
                                    links_in_row = row.find_all('a', href=True)
                                    for link in links_in_row:
                                        href = link.get('href')
                                        if href and ('download' in href or 'attach' in href):
                                            logger.info(f"      ğŸ”— ë‹¤ìš´ë¡œë“œ ë§í¬: {href}")
                
                # 3. ê³µì§€ì‚¬í•­ í˜ì´ì§€ í™•ì¸
                logger.info("\nğŸ“¢ ê³µì§€ì‚¬í•­ í˜ì´ì§€ í™•ì¸...")
                notice_url = f"https://eclass.seoultech.ac.kr/ilos/st/course/notice_list.acl?KJKEY={course_id}&"
                
                notice_response = await client.get(notice_url, cookies=cookies)
                if notice_response.status_code == 200:
                    notice_soup = BeautifulSoup(notice_response.text, 'html.parser')
                    
                    # ê³µì§€ì‚¬í•­ ë§í¬ë“¤ í™•ì¸
                    notice_detail_links = notice_soup.find_all('a', href=True)
                    detail_links = []
                    
                    for link in notice_detail_links:
                        href = link.get('href')
                        text = link.get_text(strip=True)
                        if href and 'notice_detail' in href and text:
                            detail_links.append({'text': text, 'href': href})
                    
                    logger.info(f"  ğŸ“ ê³µì§€ì‚¬í•­ ìƒì„¸ ë§í¬: {len(detail_links)}ê°œ")
                    
                    # ì²« ë²ˆì§¸ ê³µì§€ì‚¬í•­ ìƒì„¸ í™•ì¸
                    if detail_links:
                        first_notice = detail_links[0]
                        logger.info(f"  ì²« ë²ˆì§¸ ê³µì§€ì‚¬í•­: {first_notice['text']}")
                        
                        detail_url = f"https://eclass.seoultech.ac.kr{first_notice['href']}"
                        detail_response = await client.get(detail_url, cookies=cookies)
                        
                        if detail_response.status_code == 200:
                            detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                            
                            # ì²¨ë¶€íŒŒì¼ í™•ì¸
                            attach_links = detail_soup.find_all('a', href=True)
                            for link in attach_links:
                                href = link.get('href')
                                text = link.get_text(strip=True)
                                if href and ('attach' in href or 'download' in href or 'file' in href):
                                    logger.info(f"    ğŸ“ ì²¨ë¶€íŒŒì¼ ë°œê²¬: {text} -> {href}")
                
        logger.info("\nâœ… í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(analyze_eclass_structure())
